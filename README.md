# Data Processing Pipeline for Loan Default Risk

## Project Overview
In this project, I developed a scalable and efficient Data Processing Pipeline for analyzing Loan Default Risk Data from a lending company. Leveraging PySpark, this pipeline focuses on preprocessing data, conducting exploratory data analysis (EDA), and performing feature engineering.

## Major Components of the Project
- **Scalable Data Processing**: Developed a data processing pipeline using PySpark within Azure Databricks to handle large datasets efficiently.
- **Data Preprocessing and EDA**: Preprocessed data and performed exploratory data analysis to uncover insights and patterns.
- **Feature Engineering**: Conducted advanced feature engineering to enhance the predictive power of the models.
  
## Other explorations:
- **Customer Clustering**: Employed DBSCAN for clustering customers from unlabeled data, identifying distinct customer segments.
- **Active Learning and Data Augmentation**: Used active learning techniques to augment data and improve model performance.

## Tools and Technologies
- **Programming Languages**: Python, SQL
- **Libraries and Frameworks**: PySpark, scikit-learn, DBSCAN
- **Techniques**: Data preprocessing, EDA, clustering, active learning, feature engineering

## Impact
- Enhancing data scalability and handling large datasets efficiently.
- Providing insights through thorough exploratory data analysis.
- Streamlining the data processing workflow, resulting in more efficient loan default analysis.
- Saved 30% time in manual processing and analyzing the data.
