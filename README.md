# Data Processing Pipeline for Loan Default Risk

## Project Overview
The Data Processing Pipeline for Loan Default Risk is a scalable and efficient solution developed for a lending company to enhance their analysis of loan default risk. Leveraging PySpark within Azure Databricks, this project focuses on preprocessing data, conducting exploratory data analysis (EDA), clustering customers, and advanced feature engineering.

## Major Components of the Project
- **Scalable Data Processing**: Developed a data processing pipeline using PySpark within Azure Databricks to handle large datasets efficiently.
- **Data Preprocessing and EDA**: Preprocessed data and performed exploratory data analysis to uncover insights and patterns.
- **Customer Clustering**: Employed DBSCAN for clustering customers from unlabeled data, identifying distinct customer segments.
- **Active Learning and Data Augmentation**: Used active learning techniques to augment data and improve model performance.
- **Feature Engineering**: Conducted advanced feature engineering to enhance the predictive power of the models.

## Tools and Technologies
- **Programming Languages**: Python, SQL
- **Libraries and Frameworks**: PySpark, scikit-learn, DBSCAN
- **Platforms**: Azure Databricks
- **Techniques**: Data preprocessing, EDA, clustering, active learning, feature engineering

## Impact
The Data Processing Pipeline for Loan Default Risk has significantly improved the lending company's ability to analyze and predict loan defaults by:
- Enhancing data scalability and handling large datasets efficiently.
- Providing insights through thorough exploratory data analysis.
- Identifying distinct customer segments for targeted strategies.
- Improving model performance through data augmentation and advanced feature engineering.
- Streamlining the data processing workflow, resulting in more efficient loan default analysis.
- Saved 30% time in manual processing and analyzing the data
